This research project aims to perform sentiment
analysis on historical journals to gain insights
into the emotional and psychological experiences of authors, which are often overlooked in
traditional historical accounts. Specifically, we
compare the performance of three state-of-theart models for sentiment analysis: BERT, DistilBERT, and RoBERTa. Our hypothesis is that
transformer-based models, such as these, will
outperform traditional deep learning models for
sentiment analysis due to their ability to capture
complex relationships between words in a sentence. We aim to identify which model outperforms the others in terms of accuracy, precision,
recall and F1 score. Additionally, we investigate how the performance of these models is influenced by factors such as data pre-processing,
hyperparameter tuning, and model architecture.
The research findings will provide insights into
the effectiveness of these models for sentiment
analysis of historical journals, and potentially
help inform decisions about which model to
use in similar applications.

A detailed Report in included in the Repo.
